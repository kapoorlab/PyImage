{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Autoencoder\n",
    "\n",
    "We will use the MNIST dataset of handwritten digits [http://yann.lecun.com/exdb/mnist/] first to explore what autoencoders do. To begin, we need to load some python modules including common layers from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# numpy and pyplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# keras\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Conv2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the data by normalizing it.\n",
    "\n",
    "Sincle we are doing unsupervised learning here, we will not need the labels provided by the dataset for now. We keep them however, as they will help with visualizing the results later.\n",
    "\n",
    "There are 60k training and 10k test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In priciple, an autoencoder consists of two models: the encoder and the decoder. To represent this, we are using keras' functional API where we can easily define models from component models.\n",
    "\n",
    "We start by defining the encoder, whose output will be the latent space of the model. This latent space shall contain\n",
    "32 floats. Storing just this data would yield a compression factor of 24.5, as  the input consists of $28\\cdot28 = 784$ floats. Practically it is four times less, as the original images were 8-bit grayscale.\n",
    "\n",
    "Then we define the decoder, which takes the latent image as input and produces full-size images again.\n",
    "\n",
    "Finally, we chain encoder and decoder together to get our autoencoder.\n",
    "\n",
    "## Shallow Autoencoder\n",
    "First, we try a very shallow autoencoder with only one dense layer for each, the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encodingDim = 32  # \n",
    "\n",
    "# this is our input placeholder\n",
    "inputImg = Input(shape=x_train.shape[1:])\n",
    "# encoder\n",
    "x = Flatten()(inputImg)\n",
    "x = Dense(encodingDim, activation='relu')(x)\n",
    "encoder = Model(inputImg, x, name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "# this is our latent space placeholder\n",
    "inputLat = Input(shape=encoder.output.shape.as_list()[1:])\n",
    "# decoder\n",
    "x = Dense(int(np.prod(inputImg.shape[1:])), activation='sigmoid')(inputLat)\n",
    "x = Reshape(inputImg.shape[1:])(x)\n",
    "decoder = Model(inputLat, x, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(inputImg, decoder(encoder(inputImg)), name=\"generator\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model using binary crossentropy loss\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function to compare original and reconstructed images, which we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImages(ae, data):\n",
    "    decoded_imgs = ae.predict(data)\n",
    "\n",
    "    n = data.shape[0]  # how many digits we will display\n",
    "    height = 20\n",
    "    plt.figure(figsize=(height, height/n*2))\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(data[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_imgs[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "showImages(autoencoder, x_test[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron Autoencoder\n",
    "\n",
    "Let us try a deeper network. This will have much more parameters as we use dense layers only, but it will give us better reconstructions after training for 50 epochs.\n",
    "\n",
    "We will cut the latent space in half to get more compression (only use 16 float32 values), at the cost of some quality after traning only 50 epochs. So the improved reconstruction quality may not become a reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDeepMLPAE(encodingDim=16):\n",
    "    # this is our input placeholder\n",
    "    inputImg = Input(shape=x_train.shape[1:])\n",
    "    x = Flatten()(inputImg)\n",
    "    # encoder\n",
    "    x = Dense(encodingDim*16, activation='relu')(x)\n",
    "    x = Dense(encodingDim*8, activation='relu')(x)\n",
    "    x = Dense(encodingDim*4, activation='relu')(x)\n",
    "    x = Dense(encodingDim, activation='relu')(x)\n",
    "    encoder = Model(inputImg, x, name=\"encoder\")\n",
    "    encoder.summary()\n",
    "\n",
    "    # this is our latent space placeholder\n",
    "    inputLat = Input(shape=encoder.output.shape.as_list()[1:])\n",
    "    #decoder\n",
    "    x = Dense(encodingDim*4, activation='relu')(inputLat)\n",
    "    x = Dense(encodingDim*8, activation='relu')(inputLat)\n",
    "    x = Dense(encodingDim*16, activation='relu')(inputLat)\n",
    "    x = Dense(int(np.prod(inputImg.shape[1:])), activation='sigmoid')(x)\n",
    "    x = Reshape(inputImg.shape[1:])(x) # remove channel dimension\n",
    "    decoder = Model(inputLat, x, name=\"decoder\")\n",
    "    decoder.summary()\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(inputImg, decoder(encoder(inputImg)), name=\"generator\")\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder, decoder, autoencoder = makeDeepMLPAE(16)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImages(autoencoder, x_test[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstructions we obtain are still decent. Of course, longer training could perhaps improve the results. After training for another 50 epochs we would get better reconstruction than with the shallow example above.\n",
    "\n",
    "## Latent Space Representation\n",
    "\n",
    "The autoencoder effectively takes the 784 dimensional input data and tries to represent it in a 16 dimensional space (a vector of 16 floats).\n",
    "\n",
    "We can visualize the latent space only by looking at it in fewer, i.e. two, dimensions. To this end we can perfom a principal component analysis (PCA) on the encoded images, to find a transform to a coordinate system, where as much of the variance in the encoded data is happening on as few axes as possible. We can than pick the two most important ones to project all points in latent space onto this plane.\n",
    "\n",
    "We use the labels, indicating the digit to color each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "projection = pca.fit_transform(encoder.predict(x_test))\n",
    "plt.scatter(projection[:,0], projection[:,1], c=y_test, cmap=\"rainbow\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see, that in this projection, the autoencoder apparently learned to separate between ones (blue) and zeros (violet) and has everything somewhere in between.\n",
    "\n",
    "You can experiment with even smaller latent spaces, too.\n",
    "\n",
    "We can also try to project on a 3D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# try notbook mode so you can interactively rotate\n",
    "#%matplotlib notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "projection = pca.fit_transform(encoder.predict(x_test))\n",
    "p = ax.scatter(projection[:,0], projection[:,1], projection[:,2], c=y_test, cmap=\"rainbow\")\n",
    "fig.colorbar(p)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to reset back to inline after trying notebook for 3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder\n",
    "\n",
    "Deep networks work well because of their ability to abstract. Going deeper with an MLP immensely increases the number of weight needing to be trained, leading to higher training cost and eventually overfitting.\n",
    "\n",
    "So, we go to a convolutional autoencoder. We use a classical (AlexNet/GoogleNet) approach of using a set of convolutional units, extract features from the input image and use dense layers at the bottom to interpret these features. To produce an autoencoder we mirror the structure of the encoder in the decoder.\n",
    "\n",
    "In this example we just use max-pooling in the encoder, upsampling in the decoder and standard 2D convolution throughout. An alternative would be to use strided 2D convolutions in the encoder and strided transpose 2D convolutions in the decoder.\n",
    "\n",
    "Since our screen has only two dimensions, restrict the latent space to this, so we can better see what is going on. With this strong restriction we will not get as good reconstructions as before, but this more powerfull architecture should help us to still get something decent.\n",
    "\n",
    "This network has much fewer trainable parameters than the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encodingDim = 2\n",
    "\n",
    "# this is our input placeholder\n",
    "inputImg = Input(shape=x_train.shape[1:])\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "x = Reshape((*inputImg.shape.as_list()[1:],1))(inputImg)\n",
    "x = Conv2D(4, kernel_size=(3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Conv2D(4, kernel_size=(3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = Conv2D(4, kernel_size=(3, 3), activation='relu', padding=\"same\")(x)\n",
    "cshape = x.shape.as_list()[1:]\n",
    "x = Flatten()(x)\n",
    "x = Dense(encodingDim*16, activation='relu')(x)\n",
    "x = Dense(encodingDim*8, activation='relu')(x)\n",
    "x = Dense(encodingDim*4, activation='relu')(x)\n",
    "x = Dense(encodingDim, activation='relu')(x)\n",
    "encoder = Model(inputImg, x, name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "inputLat = Input(shape=encoder.output.shape.as_list()[1:])\n",
    "x = Dense(encodingDim*4, activation='relu')(inputLat)\n",
    "x = Dense(encodingDim*8, activation='relu')(inputLat)\n",
    "x = Dense(encodingDim*16, activation='relu')(inputLat)\n",
    "x = Dense(int(np.prod(cshape)), activation='relu')(x)\n",
    "x = Reshape(cshape)(x)\n",
    "x = Conv2D(4, kernel_size=(3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = UpSampling2D(size=(2,2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = Conv2D(4, kernel_size=(3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = UpSampling2D(size=(2,2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Conv2D(4, kernel_size=(3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding=\"same\")(x)\n",
    "x = Reshape(inputImg.shape.as_list()[1:])(x)\n",
    "\n",
    "decoder = Model(inputLat, x, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(inputImg, decoder(encoder(inputImg)), name=\"generator\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=40,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the latent space directly.\n",
    "\n",
    "We can see, that the encoder learned to decompse each digit into ints \"one-ness\" and its \"zero-ness\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "encoded = encoder.predict(x_test)\n",
    "plt.scatter(encoded[:,0], encoded[:,1], c=y_test, cmap=\"rainbow\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can guess from the higher loss, the reconstructions are less good then before. Notably, one can see, that this model seems to deny the existence of the digit \"4\" and always encodes these it as \"9\" instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImages(autoencoder, x_test[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train for more epochs to get slighlty better results. Doing this we can observe, that the separation between the classes in latent space improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=20,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "encoded = encoder.predict(x_test)\n",
    "plt.scatter(encoded[:,0], encoded[:,1], c=y_test, cmap=\"rainbow\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
