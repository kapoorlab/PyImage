{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats and Dogs from [CIFAR10](https://keras.io/datasets/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# keras\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# getting the datset\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load Cats and Dogs From CIFAR10 dataset **\n",
    "\n",
    "First we load the CIFAR10 data and extract all cats and dogs from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# Reduce to cats and dogs\n",
    "def extract(x,y,labels):\n",
    "    arg_train = []\n",
    "    new_y = np.zeros(np.max(labels)+1)\n",
    "    for i,l in enumerate(labels):\n",
    "        arg_train.append(np.argwhere(y == l)[:,0])\n",
    "        new_y[l] = i\n",
    "    arg_train = np.concatenate(arg_train)\n",
    "    return x[arg_train], new_y[y[arg_train]]\n",
    "    \n",
    "x_train, y_train = extract(x_train, y_train, [3,5])\n",
    "x_test, y_test = extract(x_test, y_test, [3,5])\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# maximum value normalization\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "img_rows, img_cols, channels = x_train.shape[1:]\n",
    "\n",
    "print(K.image_data_format())\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], channels, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], channels, img_rows, img_cols)\n",
    "    input_shape = (channels, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)\n",
    "    input_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "plt.imshow(x_train[np.argmax(y_train==0)])\n",
    "plt.title(\"a cat\")\n",
    "plt.show()\n",
    "plt.imshow(x_train[np.argmax(y_train==1)])\n",
    "plt.title(\"a dog\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we just define a function which will display the results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(idx, model, data, avg=None, norm=None, cols=5, threshold=.3):\n",
    "    try:\n",
    "        idx = list(idx)\n",
    "    except:\n",
    "        idx = [idx]\n",
    "        \n",
    "    cats = 0\n",
    "    dogs = 0\n",
    "    data = data[idx]\n",
    "    if norm is None:\n",
    "        p = model.predict(data)\n",
    "    else:\n",
    "        p = model.predict(data/norm)\n",
    "    i = 0\n",
    "    while i < p.shape[0]:\n",
    "        fig, axs = plt.subplots(1,cols,figsize=(5*cols,5))\n",
    "        fig.figsize=(20,10)\n",
    "        for ax in axs:\n",
    "            if avg is not None:\n",
    "                img = (data[i]+avg)\n",
    "            else:\n",
    "                img = (data[i])\n",
    "\n",
    "            ax.imshow(img)\n",
    "            if p[i] < threshold:\n",
    "                label = \"cat\"\n",
    "                cats += 1\n",
    "            elif p[i] > 1-threshold:\n",
    "                label = \"dog\"\n",
    "                dogs += 1\n",
    "            else:\n",
    "                label = \"not sure\"\n",
    "            ax.text(.5,0, label+ \"; score = \" + str(p[i]),\n",
    "                    horizontalalignment='center', verticalalignment='bottom', transform=ax.axes.transAxes,\n",
    "                    backgroundcolor=\"white\", size=\"large\")\n",
    "            i += 1\n",
    "            if i >= p.shape[0]:\n",
    "                break\n",
    "        plt.show()\n",
    "    print(cats, \" cats (\", cats/len(idx)*100., \"%),\", dogs, \" dogs (\", dogs/len(idx)*100., \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "A very simple CNN... (> 70% validation accuray after 10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(256, activation='relu'))\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = keras.optimizers.adam(lr=0.001) #keras.optimizers.Adadelta()\n",
    "\n",
    "model1.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train for ten epochs (iterations ofer the whole training data)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=10,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(range(10), model1, x_test)\n",
    "predict(range(x_test.shape[0]-10, x_test.shape[0]), model1, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rather simple CNN: Deeper than the one above, but actually with fewer trainable parameters. It gives slightly better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(48, kernel_size=(5, 5), padding=\"same\",\n",
    "                 activation='relu', input_shape=input_shape))\n",
    "model2.add(Conv2D(32, (5, 5), activation='relu', padding=\"same\"))\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Conv2D(96, (5, 5), activation='relu', padding=\"same\"))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Conv2D(160, (3, 3), activation='relu', padding=\"same\"))\n",
    "model2.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(.3))\n",
    "model2.add(Dense(512, activation='relu'))\n",
    "model2.add(Dropout(.5))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dense(1))\n",
    "model2.add(Activation('sigmoid'))\n",
    "\n",
    "opt = keras.optimizers.adam(lr=0.001)\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=10,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(range(20), model2, x_test)\n",
    "predict(range(x_test.shape[0]//2, x_test.shape[0]//2+20), model2, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Looking at the training progress we can see, that in the end the loss decreases but the validation loss does not. The network is only optimized for the training data. The test data is used to validate the performance on unseen images. What we observe here is called overfitting. This problem will be adressed in depth in the session on regularization.\n",
    "\n",
    "One simple explanation for the phenomenon in our case is, that we have limited training data over which we interate over and over and many parameters in our network which can *memorize* the data. One way to get more data is augmenting the available data with randomly transformed data which stil retains the same labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# traning data is augmented\n",
    "train_datagen = ImageDataGenerator(\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow(x_train, y_train,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the augmented training data we can continue training and improve the results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
