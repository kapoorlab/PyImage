{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization for a classifier of the cat/dog data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot functions for training evaluation\n",
    "These function will provide performance curves for each epoch of the training process. This will allow us later to judge if overfitting occured or not and if our regularization approaches were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histories(histories, keys=['loss', 'acc']):\n",
    "    fig, axs = plt.subplots(len(keys), 1, figsize=(20,16))\n",
    "    #fig.figsize=(20,16)\n",
    "    if len(keys) == 1:\n",
    "        axs = [axs]\n",
    "    # one plot for each specified key\n",
    "    for i, ax in enumerate(axs):\n",
    "        key = keys[i]\n",
    "        for name, history in histories:\n",
    "            val = ax.plot(history.epoch, history.history['val_'+key],\n",
    "                           '--', label=name.title()+' Val')\n",
    "            ax.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "                    label=name.title()+' Train')\n",
    "\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel(key.replace('_',' ').title())\n",
    "        ax.legend()\n",
    "        ax.set_title(key)\n",
    "        ax.set_xlim([0,max(history.epoch)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting input paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe have to adjust the base_dir if IO error occurs\n",
    "base_dir = os.path.join('..','..', '..', 'data','cats-dogs')\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the input pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        #shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        #horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The baseline model\n",
    "Our first model whose performance we will try to improve by means of regularization.\n",
    "We will see that it is powerful enough to classify the training data perfectly. But this is of course not our main goal. So maybe it is already to powerful (in terms of parameters)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "def get_baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_base = get_baseline_model()\n",
    "#m_base = get_baseline_model_cnn()\n",
    "print(m_base.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "m_base.compile(optimizer='rmsprop',\n",
    "               loss=\"binary_crossentropy\", \n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "hist_base = m_base.fit_generator(train_generator,\n",
    "                                 epochs=epochs,\n",
    "                                 steps_per_epoch=2000 // batch_size,\n",
    "                                 validation_data=validation_generator,\n",
    "                                 validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('baseline', hist_base)], keys=['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smaller model\n",
    "\n",
    "Since the baseline model is able to classify the training data almost perfectly, we will try to lessen its capacity by reducing the number of parameters, hoping to reduce the amount of overfitting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smaller_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # conv block with only half as many filters\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_small = get_smaller_model()\n",
    "print(m_small.summary())\n",
    "\n",
    "m_small.compile(optimizer='rmsprop',\n",
    "               loss=\"binary_crossentropy\", \n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_small = m_small.fit_generator(train_generator,\n",
    "                                   epochs=epochs,\n",
    "                                   steps_per_epoch=2000 // batch_size,\n",
    "                                   validation_data=validation_generator,\n",
    "                                   validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('small', hist_small),\n",
    "                ('baseline', hist_base), ], keys=['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## larger model\n",
    "\n",
    "By making the model larger we expect to see an even more extreme form of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_larger_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    # twice as many fc neurons here!\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    # and another dense layer!\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_large = get_larger_model()\n",
    "print(m_large.summary())\n",
    "\n",
    "m_large.compile(optimizer='rmsprop',\n",
    "                loss=\"binary_crossentropy\", \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_large = m_large.fit_generator(train_generator,\n",
    "                                   epochs=epochs,\n",
    "                                   steps_per_epoch=2000 // batch_size,\n",
    "                                   validation_data=validation_generator,\n",
    "                                   validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('small', hist_small),\n",
    "                ('baseline', hist_base), \n",
    "                ('large', hist_large)], keys=['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using \"early stopping\"\n",
    "\n",
    "The idea here is to stop the training procedure before the model starts to adapt too much on the training samples. We go on as long as the validation loss drops by at least 0.1 and stop if it does not do so during 5 consecutive training epochs.\n",
    "\n",
    "Note: This is a kind of data snooping since information of the validation set is used during training which should in general be a no-go. We show it here for demonstration only! The way to go would be to divide your data into training, validation and test set and do the early stopping based on the loss on the validation set. After the model is then trained, you should validate it on the test set which has not been looked at before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "m_base = get_baseline_model()\n",
    "\n",
    "m_base.compile(optimizer=\"rmsprop\",\n",
    "               #optimizers.Adadelta(),\n",
    "               loss=\"binary_crossentropy\", \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "hist_earlystop = m_base.fit_generator(train_generator,\n",
    "                                      epochs=epochs,\n",
    "                                      steps_per_epoch=2000 // batch_size,\n",
    "                                      validation_data=validation_generator,\n",
    "                                      validation_steps=800 // batch_size,\n",
    "                                      callbacks=[\n",
    "                                          EarlyStopping(monitor='val_loss', min_delta=0.1, patience=5, verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('early_stopping', hist_earlystop), \n",
    "                ('baseline', hist_base)], keys=['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight regularisation\n",
    "\n",
    "You may be familiar with Occam's Razor principle: given two explanations for something, the explanation most likely to be correct is the \"simplest\" one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.\n",
    "\n",
    "A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more \"regular\". This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n",
    "\n",
    "* L1 regularization, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n",
    "\n",
    "* L2 regularization, where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the \"L2 norm\" of the weights). L2 regularization is also called weight decay in the context of neural networks. Don't let the different name confuse you: weight decay is mathematically the exact same as L2 regularization.\n",
    "\n",
    "In Keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Let's try both flavors now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy-paste the baseline model, but add regularization terms to each layer\n",
    "from keras import regularizers\n",
    "\n",
    "reg_weight = 1.e-4\n",
    "\n",
    "def get_l1_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l1(reg_weight), input_shape=(150, 150, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l1(reg_weight)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l1(reg_weight)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l1(reg_weight)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1, kernel_regularizer=regularizers.l1(reg_weight)))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```l1(0.001)``` means that every coefficient in the weight matrix of the layer will add ```0.001 * abs(weight_coefficient_value)``` to the total loss of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_l1 = get_l1_model()\n",
    "\n",
    "m_l1.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"binary_crossentropy\", \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "hist_l1 = m_l1.fit_generator(train_generator,\n",
    "                                 epochs=epochs,\n",
    "                                 steps_per_epoch=2000 // batch_size,\n",
    "                                 validation_data=validation_generator,\n",
    "                                 validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('l1_reg', hist_l1), \n",
    "                ('baseline', hist_base)], keys=['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because this penalty is only added at training time, the loss for this network will be higher at training than at test time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy-paste the baseline model, but add regularization terms to each layer\n",
    "from keras import regularizers\n",
    "\n",
    "reg_weight = 1.e-4\n",
    "\n",
    "def get_l2_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(reg_weight), input_shape=(150, 150, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(reg_weight)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(reg_weight)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l2(reg_weight)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1, kernel_regularizer=regularizers.l2(reg_weight)))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_l2 = get_l2_model()\n",
    "\n",
    "m_l2.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"binary_crossentropy\", \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "hist_l2 = m_l2.fit_generator(train_generator,\n",
    "                             epochs=epochs,\n",
    "                             steps_per_epoch=2000 // batch_size,\n",
    "                             validation_data=validation_generator,\n",
    "                             validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('l2_reg', hist_l2), \n",
    "                ('baseline', hist_base)], keys=['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('l1_reg', hist_l1), \n",
    "                ('baseline', hist_base),\n",
    "                ('l2_reg', hist_l2)], keys=['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_weight = 1.e-4\n",
    "\n",
    "def get_l1_l2_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l1_l2(reg_weight, reg_weight), \n",
    "                     input_shape=(150, 150, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l1_l2(reg_weight, reg_weight)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l1_l2(reg_weight, reg_weight)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l1_l2(reg_weight, reg_weight)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1, kernel_regularizer=regularizers.l1_l2(reg_weight, reg_weight)))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_l1l2 = get_l1_l2_model()\n",
    "\n",
    "m_l1l2.compile(optimizer=\"rmsprop\",\n",
    "               loss=\"binary_crossentropy\", \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "hist_l1l2 = m_l1l2.fit_generator(train_generator,\n",
    "                                 epochs=epochs,\n",
    "                                 steps_per_epoch=2000 // batch_size,\n",
    "                                 validation_data=validation_generator,\n",
    "                                 validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('l1_l2_reg', hist_l1l2), \n",
    "                ('baseline', hist_base)], keys=['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('l1_reg', hist_l1), \n",
    "                ('baseline', hist_base),\n",
    "                ('l2_reg', hist_l2),\n",
    "                ('l1_l2_reg', hist_l1l2)], keys=['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using dropout\n",
    "\n",
    "Dropout is one of the most commonly used regularization techniques for neural networks, developed by Hinton and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. set to zero) a number of output features of the layer during training. Let's say a given layer would normally have returned a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. [0, 0.5, \n",
    "1.3, 0, 1.1]. The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time.\n",
    "\n",
    "In tf.keras you can introduce dropout in a network via the Dropout layer, which gets applied to the output of layer right before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "def get_dropout_model(rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_drop = get_dropout_model()\n",
    "\n",
    "m_drop.compile(optimizer=\"rmsprop\",\n",
    "               loss=\"binary_crossentropy\", \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "hist_drop = m_drop.fit_generator(train_generator,\n",
    "                                 epochs=epochs,\n",
    "                                 steps_per_epoch=2000 // batch_size,\n",
    "                                 validation_data=validation_generator,\n",
    "                                 validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('dropout', hist_drop), \n",
    "                ('baseline', hist_base)], keys=['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the effect of a higher dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_more_drop = get_dropout_model(0.75)\n",
    "m_more_drop.compile(optimizer=\"rmsprop\",\n",
    "                    loss=\"binary_crossentropy\", \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "hist_more_drop = m_more_drop.fit_generator(train_generator,\n",
    "                                           epochs=epochs,\n",
    "                                           steps_per_epoch=2000 // batch_size,\n",
    "                                           validation_data=validation_generator,\n",
    "                                           validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('dropout', hist_drop), \n",
    "                ('baseline', hist_base),\n",
    "                ('more_dropout', hist_more_drop)], keys=['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using batch normalization\n",
    "\n",
    "Intuitively you want the variances of each layers input of your network to be nice. You do not want them to go to zero nor do you want them to explode. Ideally you want everything be normalized throughout your network. \n",
    "Batch normalization is a new technique that does this: it just takes the batch mean and standard deviation of each of our features (i.e. for each pixel across the data) and then sets its value to be such that it has zero mean and unit variance.\n",
    "\n",
    "It turns out that this function is differentiable and we can backpropagate it! By inserting now a batch normalization layer in our network we will take in as input some x and normalize it. So for a given input activation we can apply this normalization independently across our batch. So the hope is that by doing this the network will learn faster because it will not have to adapt to changes in the distribution of the inputs it receives.\n",
    "\n",
    "Benefits of BN\n",
    "\n",
    "- Improves gradient flow through network\n",
    "- Allow higher learning rates\n",
    "- Reduce strong dependency on initialization\n",
    "- Acts as a form of regularization\n",
    "- Note: \n",
    "    - at test time BN layer works differently\n",
    "    - Mean and Std not computed based on batch (since you might want to predict single images)\n",
    "    - Use a running mean and std. (maintained during training)\n",
    "\n",
    "One more subtle issue is that it actually acts as a form of regularization. \n",
    "The way it acts as a regularizer is that with BN, if you have some input x, and it goes through the network then its representation in the network is also influenced by all the other examples in your batch since we use the batch mean and variance. \n",
    "So BN is using all the images of the batch a particular image was sampled in. So it randomizes the images position in the representation space which has a nice regularizing effect.\n",
    "\n",
    "Note that at test time the BN layer acts differently. At test time, we want it to be a deterministic function. So we need to remember our mean and variance across the dataset. We can either compute it over all our data or much easier is to have a running mean and variance while training and then use that directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def get_batchnorm_model():    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_bn = get_batchnorm_model()\n",
    "\n",
    "m_bn.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"binary_crossentropy\", \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# we use only about half as many iterations since batch norm makes learning quicker\n",
    "hist_bn = m_bn.fit_generator(train_generator,\n",
    "                             epochs=int(0.5 * epochs),\n",
    "                             steps_per_epoch=2000 // batch_size,\n",
    "                             validation_data=validation_generator,\n",
    "                             validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('batch_norm', hist_bn), \n",
    "                ('baseline', hist_base)], keys=['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using data augmentation\n",
    "\n",
    "In order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that our model would never see the exact same picture twice. This helps prevent overfitting and helps the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to set up a different input pipeline that does the augmentation for us\n",
    "train_datagen_aug = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=45,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "train_generator_aug = train_datagen_aug.flow_from_directory(\n",
    "        train_dir,  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some augmented images: on every execution you should get different images\n",
    "img_batch, label_batch = train_generator_aug.next()\n",
    "n_per_row = 3\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(n_per_row):\n",
    "    plt.subplot(2, n_per_row, i + 1)\n",
    "    plt.imshow(img_batch[i])\n",
    "    plt.title(\"class \" + str(int(label_batch[i])))\n",
    "\n",
    "for i in range(n_per_row):\n",
    "    plt.subplot(2, n_per_row, n_per_row + i + 1)\n",
    "    plt.imshow(img_batch[n_per_row + i])\n",
    "    plt.title(\"class \" + str(int(label_batch[n_per_row + i])))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during validation we do NOT augment the images of course\n",
    "test_datagen_aug = ImageDataGenerator(rescale=1. / 255)\n",
    "test_generator_aug = test_datagen_aug.flow_from_directory(\n",
    "        validation_dir,  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_base = get_baseline_model()\n",
    "\n",
    "m_base.compile(optimizer=\"rmsprop\",\n",
    "               loss=\"binary_crossentropy\", \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "hist_aug = m_base.fit_generator(train_generator_aug,\n",
    "                                epochs=epochs,\n",
    "                                steps_per_epoch=2000 // batch_size,\n",
    "                                validation_data=test_generator_aug,\n",
    "                                validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories([('augmentation', hist_aug),\n",
    "                ('baseline', hist_base)], keys=[\"loss\"])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
