{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with Keras\n",
    "\n",
    "\n",
    "In this notebook, we will present an introduction to Transfer Learning which are a few simple yet effective methods that you can use to build a powerful image classifier, using only very few training examples --- just a few hundred or thousand pictures from each class you want to be able to recognize --- by utilizing pretrained models. \n",
    "\n",
    "We will go over the following options:\n",
    "\n",
    "* training a small network from scratch (as a baseline)\n",
    "* using the bottleneck features of a pre-trained network\n",
    "* fine-tuning the top layers of a pre-trained network\n",
    "\n",
    "By the end you will know how to use the following Keras APIs:\n",
    "\n",
    "* `ImageDataGenerator` for real-time data augmentation\n",
    "*  `fit_generator` for training a Keras model using Python data generators\n",
    "* layer freezing and model fine-tuning\n",
    "* ...and more!\n",
    "\n",
    "## Cats-Dogs Dataset\n",
    "\n",
    "We will start from a setup of only 2000 training examples (1000 per class either a dog or a cat). The data is stored in the `data/cats-dogs` folder which has a `train` data directory and a `validation` data directory containing one subdirectory per image class, filled with `.jpg` images:\n",
    "\n",
    "```\n",
    "cats-dogs/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog.001.jpg\n",
    "            dog.002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat.001.jpg\n",
    "            cat.002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog.003.jpg\n",
    "            dog.004.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat.003.jpg\n",
    "            cat.004.jpg\n",
    "            ...\n",
    "```\n",
    "\n",
    "The `validataion` set consists of 400 additional samples from each class as validation data, to evaluate our models.\n",
    "\n",
    "Note that `2000` samples are *very* few examples to learn from, for a classification problem that is far from simple. So this is a challenging machine learning problem, but it is also a realistic one: in a lot of real-world use cases, even small-scale data collection can be extremely expensive or sometimes near-impossible (e.g. in medical imaging). Being able to make the most out of very little data is a key skill of a competent data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join('..','..','data','cats-dogs')\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the data looks like:\n",
    "\n",
    "![](../../images/dogs-cats.jpg)\n",
    "\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "You might have heard that  \"deep learning is only relevant when you have a huge amount of data\". While not entirely incorrect, this is somewhat misleading. Certainly, deep learning requires the ability to learn features automatically from the data, which is generally only possible when lots of training data is available --- especially for problems where the input samples are very high-dimensional, like images. However, convolutional neural networks --- a pillar algorithm of deep learning--- are by design one of the best models available for most \"perceptual\" problems (such as image classification), even with very little data to learn from. Training a convnet from scratch on a small image dataset will still yield reasonable results, without the need for any custom feature engineering. Convnets are just plain good. They are the right tool for the job.\n",
    "\n",
    "But what's more, deep learning models are by nature highly repurposable: you can take, say, an image classification or speech-to-text model trained on a large-scale dataset then reuse it on a significantly different problem with only minor changes, as we will see in this tutorial. Specifically in the case of computer vision, many pre-trained models (usually trained on the ImageNet dataset) are now publicly available for download and can be used to bootstrap powerful vision models out of very little data.\n",
    "\n",
    "## Data pre-processing and data augmentation\n",
    "\n",
    "In order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that our model would never see the exact same picture twice. This helps prevent overfitting and helps the model generalize better.\n",
    "\n",
    "In Keras this can be done via the `keras.preprocessing.image.ImageDataGenerator` class. This class allows you to:\n",
    "\n",
    "* configure random transformations and normalization operations to be done on your image data during training\n",
    "* instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`. \n",
    "\n",
    "These generators can then be used with the Keras model methods that accept data generators as inputs, `fit_generator`, `evaluate_generator` and `predict_generator`.\n",
    "\n",
    "Let's look at an example right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few of the options available (for more, see [the documentation](http://keras.io/preprocessing/image/)). Let's quickly go over what we just wrote:\n",
    "\n",
    "* `rotation_range` is a value in degrees (0-180), a range within which to randomly rotate pictures\n",
    "* `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally\n",
    "* `rescale` is a value by which we will multiply the data before any other processing. Our original images consist in RGB coefficients in the 0-255, but such values would be too high for our models to process (given a typical learning rate), so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n",
    "* `shear_range` is for randomly applying [shearing transformations](https://en.wikipedia.org/wiki/Shear_mapping)\n",
    "* `zoom_range` is for randomly zooming inside pictures\n",
    "* `horizontal_flip` is for randomly flipping half of the images horizontally ---relevant when there are no assumptions of horizontal assymetry (e.g. real-world pictures).\n",
    "* `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n",
    "\n",
    "Now let's start generating some pictures using this tool and save them to a temporary directory, so we can get a feel for what our augmentation strategy is doing ---we disable rescaling in this case to keep the images displayable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "img = load_img(os.path.join(train_cats_dir, 'cat.10008.jpg'))  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (374, 500, 3)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 374, 500, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.flow()` command below generates batches of randomly transformed images and it will loop indefinitely, so we need to `break` the loop at some point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break  # otherwise the generator would loop indefinitely\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: A small ConvNet\n",
    "\n",
    "The right tool for an image classification job is a convnet, so let's try to train one on our data, as an initial baseline. Since we only have few examples, our number one concern should be *overfitting*. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. For instance, if you, as a human, only see three images of people who are lumberjacks, and three, images of people who are sailors, and among them only one lumberjack wears a cap, you might start thinking that wearing a cap is a sign of being a lumberjack as opposed to a sailor. You would then make a pretty lousy lumberjack/sailor classifier.\n",
    "\n",
    "Data augmentation is one way to fight overfitting, but it isn't enough since our augmented samples are still highly correlated. Your main focus for fighting overfitting should be the entropic capacity of your model ---how much information your model is allowed to store. A model that can store a lot of information has the potential to be more accurate by leveraging more features, but it is also more at risk to start storing irrelevant features. Meanwhile, a model that can only store a few features will have to focus on the most significant features found in the data, and these are more likely to be truly relevant and to generalize better.\n",
    "\n",
    "There are different ways to modulate entropic capacity. The main one is the choice of the number of parameters in your model, i.e. the number of layers and the size of each layer. Another way is the use of weight regularization, such as L1 or L2 regularization, which consists in forcing model weights to taker smaller values.\n",
    "\n",
    "In our case we will use a very small convnet with few layers and few filters per layer, alongside data augmentation and dropout. Dropout also helps reduce overfitting, by preventing a layer from seeing the exact same pattern twice, thus acting in a way analoguous to data augmentation (you could say that both dropout and data augmentation tend to disrupt random correlations occuring in your data).\n",
    "\n",
    "The code snippet below is our first model, a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers. This is very similar to the architectures that Yann LeCun advocated in the 1990s for image classification (with the exception of ReLU), but don't worry about the details, we will cover them all soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "          input_shape=(150, 150, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model so far outputs 3D feature maps `(height, width, features)`.  On top of it we stick two fully-connected layers. We end the model with a single unit and a sigmoid activation, which is perfect for a binary classification. To go with it we will also use the `binary_crossentropy` loss to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten()) # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our data. We will use `.flow_from_directory()` to generate batches of image data (and their labels) directly from our images in their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these generators to train our model. Each epoch takes 15-20s on GPU and 300-400s on CPU. So it's definitely viable to run this model on CPU if you aren't in a hurry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.7434 - acc: 0.5145 - val_loss: 0.6905 - val_acc: 0.5012\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.6970 - acc: 0.5605 - val_loss: 0.7124 - val_acc: 0.5750\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.6659 - acc: 0.6270 - val_loss: 0.6327 - val_acc: 0.6025\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.6628 - acc: 0.6270 - val_loss: 0.6310 - val_acc: 0.6550\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.6334 - acc: 0.6440 - val_loss: 0.6040 - val_acc: 0.6450\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.6120 - acc: 0.6800 - val_loss: 0.5862 - val_acc: 0.6813\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.5938 - acc: 0.6880 - val_loss: 0.6003 - val_acc: 0.6975\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.5816 - acc: 0.6845 - val_loss: 0.6047 - val_acc: 0.6587\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.5717 - acc: 0.7140 - val_loss: 0.5575 - val_acc: 0.7087\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.5598 - acc: 0.7275 - val_loss: 0.5839 - val_acc: 0.6987\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.5399 - acc: 0.7380 - val_loss: 0.5764 - val_acc: 0.7212\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.5228 - acc: 0.7455 - val_loss: 0.6511 - val_acc: 0.6925\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.5234 - acc: 0.7485 - val_loss: 0.5232 - val_acc: 0.7425\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.5028 - acc: 0.7680 - val_loss: 0.4942 - val_acc: 0.7562\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.4970 - acc: 0.7710 - val_loss: 0.4776 - val_acc: 0.7688\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.4638 - acc: 0.7820 - val_loss: 0.5218 - val_acc: 0.7600\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.4736 - acc: 0.7830 - val_loss: 0.4898 - val_acc: 0.7638\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.4508 - acc: 0.8005 - val_loss: 0.5477 - val_acc: 0.7488\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.4544 - acc: 0.8015 - val_loss: 0.4538 - val_acc: 0.7712\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.4282 - acc: 0.8105 - val_loss: 0.4495 - val_acc: 0.7825\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.4297 - acc: 0.8025 - val_loss: 0.5285 - val_acc: 0.7700\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.4095 - acc: 0.8160 - val_loss: 0.4447 - val_acc: 0.7975\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.4048 - acc: 0.8240 - val_loss: 0.4668 - val_acc: 0.7925\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.4057 - acc: 0.8285 - val_loss: 0.4776 - val_acc: 0.7963\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.3757 - acc: 0.8370 - val_loss: 0.4561 - val_acc: 0.8063\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3798 - acc: 0.8385 - val_loss: 0.5520 - val_acc: 0.7850\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3880 - acc: 0.8395 - val_loss: 0.4027 - val_acc: 0.8225\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.3695 - acc: 0.8465 - val_loss: 0.4134 - val_acc: 0.8075\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3746 - acc: 0.8475 - val_loss: 0.9235 - val_acc: 0.6462\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3731 - acc: 0.8415 - val_loss: 0.5698 - val_acc: 0.7625\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.3578 - acc: 0.8490 - val_loss: 0.5633 - val_acc: 0.7400\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.3620 - acc: 0.8550 - val_loss: 0.5545 - val_acc: 0.7575\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 15s 120ms/step - loss: 0.3486 - acc: 0.8595 - val_loss: 0.4264 - val_acc: 0.8237\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.3633 - acc: 0.8490 - val_loss: 0.4360 - val_acc: 0.8163\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.3532 - acc: 0.8645 - val_loss: 0.4872 - val_acc: 0.8000\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3459 - acc: 0.8575 - val_loss: 0.4839 - val_acc: 0.7963\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3342 - acc: 0.8690 - val_loss: 0.5048 - val_acc: 0.7825\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3366 - acc: 0.8590 - val_loss: 0.7016 - val_acc: 0.7825\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3432 - acc: 0.8640 - val_loss: 0.4742 - val_acc: 0.8013\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 15s 120ms/step - loss: 0.3357 - acc: 0.8660 - val_loss: 0.4711 - val_acc: 0.8150\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3165 - acc: 0.8700 - val_loss: 0.6601 - val_acc: 0.8087\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3325 - acc: 0.8680 - val_loss: 0.6163 - val_acc: 0.8137\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 15s 120ms/step - loss: 0.3080 - acc: 0.8670 - val_loss: 0.4397 - val_acc: 0.8275\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3122 - acc: 0.8705 - val_loss: 0.4439 - val_acc: 0.8375\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 15s 120ms/step - loss: 0.3276 - acc: 0.8685 - val_loss: 0.4008 - val_acc: 0.8387\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 15s 120ms/step - loss: 0.3251 - acc: 0.8700 - val_loss: 0.6876 - val_acc: 0.7275\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3243 - acc: 0.8750 - val_loss: 0.4643 - val_acc: 0.8213\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.3301 - acc: 0.8660 - val_loss: 0.4292 - val_acc: 0.8163\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.3047 - acc: 0.8830 - val_loss: 0.5302 - val_acc: 0.8213\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.3103 - acc: 0.8775 - val_loss: 0.6812 - val_acc: 0.7837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4ebd0a3eb8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach gets us to a validation accuracy of 0.79-0.81 after 50 epochs (a number that was picked arbitrarily --- because the model is small and uses aggressive dropout, it does not seem to be overfitting too much by that point). \n",
    "\n",
    "Note that the variance of the validation accuracy is fairly high, both because accuracy is a high-variance metric and because we only use 800 validation samples. A good validation strategy in such cases would be to do k-fold cross-validation, but this would require training k models for every evaluation round.\n",
    "\n",
    "Let's save our model --- we will be using it to visualize the network later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cats_and_dogs_small.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bottleneck Features\n",
    "\n",
    "A more refined approach would be to leverage a network pre-trained on a large dataset. Such a network would have already learned features that are useful for most computer vision problems, and leveraging such features would allow us to reach a better accuracy than any method that would only rely on the available data.\n",
    "\n",
    "We will use the VGG16 architecture, pre-trained on the ImageNet dataset ---a model we will learn about later. Because the ImageNet dataset contains several \"cat\" classes (persian cat, siamese cat, ...) and many \"dog\" classes among its total of 1000 classes, this model will already have learned features that are relevant to our classification problem. In fact, it is possible that merely recording the softmax predictions of the model over our data rather than the bottleneck features would be enough to solve our dogs vs. cats classification problem extremely well. However, the method we present here is more likely to generalize well to a broader range of problems, including problems featuring classes absent from ImageNet.\n",
    "\n",
    "Here's what the VGG16 architecture looks like:\n",
    "\n",
    "![](../../images/vgg16_original.png)\n",
    "\n",
    "Our strategy will be as follow: we will only instantiate the convolutional part of the model, everything up to the fully-connected layers. We will then run this model on our training and validation data once, recording the output (the \"bottleneck features\" from the VGG16 model: the last activation maps before the fully-connected layers) as numpy arrays. Then we will train a small fully-connected model on top of these stored features.\n",
    "\n",
    "The reason why we are storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing, is computational efficiency. Running VGG16 is expensive, especially if you're working on CPU, and we want to only do it once. Note that this prevents us from using data augmentation though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "\n",
    "model = applications.VGG16(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_dir, \n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "        shuffle=False) # our data will be in order, so all first 1000 images will be cats, then 1000 dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train = model.predict_generator(\n",
    "        train_generator, 2000 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_validation = model.predict_generator(\n",
    "        validation_generator, 800 // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these features to train a small fully-connected model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the features were saved in order, so recreating the labels is easy\n",
    "train_labels = np.array([0] * 1000 + [1] * 1000)\n",
    "validation_labels = np.array([0] * 400 + [1] * 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=bottleneck_features_train.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to its small size, this model trains very quickly even on CPU (1s per epoch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(bottleneck_features_train, train_labels,\n",
    "          epochs=50,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(bottleneck_features_validation, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach a validation accuracy of 0.90-0.91: not bad at all. This is definitely partly due to the fact that the base model was trained on a dataset that already featured dogs and cats (among hundreds of other classes).\n",
    "\n",
    "We also save the weights of this model as we will use it for our next method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('bottleneck_fc_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "To further improve our previous result, we can try to \"fine-tune\" the last convolutional block of the VGG16 model alongside the top-level classifier. Fine-tuning consist in starting from a trained network, then re-training it on a new dataset using very small weight updates. In our case, this can be done in 3 steps:\n",
    "\n",
    "*  instantiate the convolutional base of VGG16 and load its weights\n",
    "* add our previously defined fully-connected model on top, and load its weights\n",
    "* freeze the layers of the VGG16 model up to the last convolutional block\n",
    "\n",
    "![](../../images/vgg16_modified.png)\n",
    "\n",
    "Note that:\n",
    "* in order to perform fine-tuning, all layers should start with properly trained weights: for instance you should not slap a randomly initialized fully-connected network on top of a pre-trained convolutional base. This is because the large gradient updates triggered by the randomly initialized weights would wreck the learned weights in the convolutional base. In our case this is why we first train the top-level classifier, and only then start fine-tuning convolutional weights alongside it.\n",
    "* we choose to only fine-tune the last convolutional block rather than the entire network in order to prevent overfitting, since the entire network would have a very large entropic capacity and thus a strong tendency to overfit. The features learned by low-level convolutional blocks are more general, less abstract than those found higher-up, so it is sensible to keep the first few blocks fixed (more general features) and only fine-tune the last one (more specialized features).\n",
    "* fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays very small, so as not to wreck the previously learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = applications.VGG16(input_shape=(150,150,3),\n",
    "                           weights='imagenet',\n",
    "                           include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After instantiating the VGG base and loading its weights, we add our previously trained fully-connected classifier on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its necessary to start with a fully-trained classifier including the top classifier to do fine-tuning. Thus we now load the weights we saved in the bottelneck method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model.load_weights('bottleneck_fc_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the model on top of the VGG16 convolutional base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model = Model(inputs=vgg16.input, \n",
    "              outputs=top_model(vgg16.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to freeze all convolutional layers up to the last convolutional block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start training the whole thing, with a very slow learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=2000 // batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach gets us to a validation accuracy of 0.94 after 50 epochs. Great success!\n",
    "\n",
    "Here are a few more approaches you can try to get to above 0.95:\n",
    "* more aggresive data augmentation\n",
    "* more aggressive dropout\n",
    "* use of L1 and L2 regularization (also known as \"weight decay\")\n",
    "* fine-tuning one more convolutional block (alongside greater regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
