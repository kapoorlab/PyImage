{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Cat/Dog classifier\n",
    "\n",
    "Here we will try to train a cat/dog classifiers with images of higher-resolution than we got from the CIFA10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# keras\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "First we load some prepared images of cats and dogs and split the data into training and test sets and make sure we have equal numbers of cats and dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE=\"../../data/catsDogsImageNet.npz\"\n",
    "with np.load(DATAFILE) as datafile:\n",
    "    ncats = np.where(datafile[\"label\"]==0)[0].size\n",
    "    ndogs = np.where(datafile[\"label\"]==1)[0].size\n",
    "    neach = min(ncats, ndogs)\n",
    "    print(datafile[\"data\"].shape[0], \" images in total\")\n",
    "    print(ncats, \" cats, \", ndogs, \" dogs; using \", neach, \" of each\")\n",
    "    data = np.concatenate((datafile['data'][0:neach], datafile['data'][ncats:ncats+neach]))\n",
    "    data = data /255.\n",
    "    dataAverage = np.average(data, axis=0)\n",
    "    #data -= dataAverage\n",
    "    labels = np.concatenate((np.zeros(neach), np.ones(neach)))\n",
    "    \n",
    "    plt.imshow(data[0])\n",
    "    plt.title(\"a cat\")\n",
    "    plt.show()\n",
    "    plt.imshow(data[ncats])\n",
    "    plt.title(\"a dog\")\n",
    "    plt.show()\n",
    "    \n",
    "selection = np.random.permutation(data.shape[0])\n",
    "split = int(len(selection)*.7)\n",
    "x_train = data[selection[:split]]\n",
    "y_train = labels[selection[:split]]\n",
    "x_test = data[selection[split:]]\n",
    "y_test = labels[selection[split:]]\n",
    "del labels\n",
    "\n",
    "# traning data is augmented\n",
    "train_datagen = ImageDataGenerator(\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "batch_size=32\n",
    "input_shape=x_train.shape[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the we only have ~4500 images and these contain more potentially useless information as they are larger. To metigate this to an extent we use data augmentation from the start.\n",
    "\n",
    "The `.flow()` command below generates batches of randomly transformed images and it will loop indefinitely, so we need to `break` the loop at some point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(x_train, y_train,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for batch in train_generator:\n",
    "    for i in range(batch[0].shape[0]):\n",
    "        plt.figure(i)\n",
    "        imgplot = plt.imshow(batch[0][i])\n",
    "    break  # otherwise the generator would loop indefinitely\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next Cell defines parameters we use for training our networks and defines functions which we will use later to print our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(idx, model, avg=None, transform = lambda x : x/255., cols=3, file=DATAFILE, data=None, threshold=.4):\n",
    "    try:\n",
    "        idx = list(idx)\n",
    "    except:\n",
    "        idx = [idx]\n",
    "        \n",
    "    cats = 0\n",
    "    dogs = 0\n",
    "    if data is None:\n",
    "        with np.load(file) as datafile:\n",
    "            data = datafile[\"data\"][idx]\n",
    "    else:\n",
    "        data = data[idx]\n",
    "    \n",
    "    if transform is None:\n",
    "        transform = lambda x : x\n",
    "    p = model.predict(transform(data))\n",
    "    i = 0\n",
    "    while i < p.shape[0]:\n",
    "        fig, axs = plt.subplots(1,cols,figsize=(8*cols,8))\n",
    "        fig.figsize=(20,10)\n",
    "        for ax in axs:\n",
    "            if avg is not None:\n",
    "                img = (data[i]+avg)\n",
    "            else:\n",
    "                img = (data[i])\n",
    "\n",
    "            ax.imshow(img)\n",
    "            if p[i] < threshold:\n",
    "                label = \"cat\"\n",
    "                cats += 1\n",
    "            elif p[i] > 1-threshold:\n",
    "                label = \"dog\"\n",
    "                dogs += 1\n",
    "            else:\n",
    "                label = \"not sure\"\n",
    "            ax.text(.5,0, label+ \"; score = \" + str(p[i]),\n",
    "                    horizontalalignment='center', verticalalignment='bottom', transform=ax.axes.transAxes,\n",
    "                    backgroundcolor=\"white\", size=\"large\")\n",
    "            i += 1\n",
    "            if i >= p.shape[0]:\n",
    "                break\n",
    "        plt.show()\n",
    "    print(cats, \" cats (\", cats/len(idx)*100., \"%),\", dogs, \" dogs (\", dogs/len(idx)*100., \"%)\")\n",
    "    \n",
    "def predictCategorical(idx, model, decode=lambda x : str(x), preproc=lambda x : x/255.\n",
    "                       , cols=3, file=DATAFILE, data=None):\n",
    "    try:\n",
    "        idx = list(idx)\n",
    "    except:\n",
    "        idx = [idx]\n",
    "        \n",
    "    if data is None:\n",
    "        with np.load(file) as datafile:\n",
    "            data = datafile[\"data\"][idx]\n",
    "    else:\n",
    "        data = data[idx]\n",
    "        \n",
    "    p = model.predict(data if preproc is None else preproc(data))\n",
    "    \n",
    "    i=0\n",
    "    while i < p.shape[0]:\n",
    "        fig, axs = plt.subplots(1,cols,figsize=(8*cols,8))\n",
    "        fig.figsize=(20,10)\n",
    "        for ax in axs:\n",
    "            ax.imshow(data[i])\n",
    "            ax.text(.5,0, decode(p[i:i+1]),\n",
    "                    horizontalalignment='center', verticalalignment='bottom', transform=ax.axes.transAxes,\n",
    "                    backgroundcolor=\"white\", size=\"large\")\n",
    "            i += 1\n",
    "            if i >= p.shape[0]:\n",
    "                break\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Rather Simple Network\n",
    "\n",
    "Next, we define an train a rather simple network to tell cats from dogs. We used this one before on the CIFAR 10 data and it got us to about 80% accuracy there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), input_shape=input_shape))\n",
    "model.add(Conv2D(48, kernel_size=(5, 5),\n",
    "                 activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(32, (5, 5), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(96, (5, 5), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(160, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "opt = keras.optimizers.adam(lr=0.0001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=4000 // batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=(x_test, y_test))\n",
    "\n",
    "# model.save_weights('weights.h5')\n",
    "# model.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ten epochs we see a validation accuray a bit over 70% and a few more epoch may get this up to ~80% again.\n",
    "\n",
    "Now, let us have a look at the results this network give on a few images and the corresponding predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some arbitrary selection from the data\n",
    "predict([1,2000,3000,4000,4300,4510], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is not very good, but decent, given, that some of the training images are rather complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some of the training images\n",
    "predict(range(9), model, data=x_train, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some of the validation images\n",
    "predict(range(9), model, data=x_test, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Pretrained Network\n",
    "\n",
    "We will now try to use a pre-trained network for our purposes. The network we will try is the inception_v3 net trained on the ImageNet Dataset. This dataset contains images from 1000 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import inception_v3, VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inception = inception_v3.InceptionV3()\n",
    "\n",
    "predictCategorical([1,2000,3000,4000,4300,4510], model=inception, decode=lambda x: inception_v3.decode_predictions(x, top=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Recent winners of the ImageNet competitions are very accurate. Can we change it to only do this and to also focus on cats and dogs only?\n",
    "\n",
    "One way in which they are better than our small network is that they are able to generalize concepts better. The deep networks have learned to extract features from image which make up the things it knows. By replacing the top layer of the network by one we train ourselfes we can make use of the pre-trained network's ability to extract features to distinguish only the classes we want it to:\n",
    "\n",
    "## Using Bottleneck Features\n",
    "\n",
    "We will use the VGG16 architecture, pre-trained on the ImageNet dataset ---a model we will learn about later. Because the ImageNet dataset contains several \"cat\" classes (persian cat, siamese cat, ...) and many \"dog\" classes among its total of 1000 classes, this model will already have learned features that are relevant to our classification problem. In fact, it is possible that merely recording the softmax predictions of the model over our data rather than the bottleneck features would be enough to solve our dogs vs. cats classification problem extremely well. However, the method we present here is more likely to generalize well to a broader range of problems, including problems featuring classes absent from ImageNet.\n",
    "\n",
    "Here's what the VGG16 architecture looks like:\n",
    "\n",
    "![](../../images/vgg16_original.png)\n",
    "\n",
    "Our strategy will be as follow: we will only instantiate the convolutional part of the model, everything up to the fully-connected layers. We will then run this model on our training and validation data once, recording the output (the \"bottleneck features\" from the VGG16 model: the last activation maps before the fully-connected layers) as numpy arrays. Then we will train a small fully-connected model on top of these stored features.\n",
    "\n",
    "The reason why we are storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing, is computational efficiency. Running VGG16 is expensive, especially if you're working on CPU, and we want to only do it once. Note that this prevents us from using data augmentation though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = VGG16(include_top=False, weights='imagenet',input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000 // batch_size\n",
    "\n",
    "x_bottleneck_features_train = []\n",
    "y_bottleneck_features_train = []\n",
    "for i in train_generator:\n",
    "    x_bottleneck_features_train.append(vgg16.predict(i[0]))\n",
    "    y_bottleneck_features_train.append(i[1])\n",
    "    n -= 1\n",
    "    if n <= 0:\n",
    "        break\n",
    "x_bottleneck_features_train = np.concatenate(x_bottleneck_features_train)\n",
    "y_bottleneck_features_train = np.concatenate(y_bottleneck_features_train)\n",
    "\n",
    "x_bottleneck_features_test = vgg16.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=x_bottleneck_features_train.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_bottleneck_features_train, y_bottleneck_features_train,\n",
    "          epochs=10,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(x_bottleneck_features_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach a validation accuracy of 0.90-0.91: not bad at all. This is definitely partly due to the fact that the base model was trained on a dataset that already featured dogs and cats (among hundreds of other classes).\n",
    "\n",
    "We also save the weights of this model as we will use it for our next method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('bottleneck_fc_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "To further improve our previous result, we can try to \"fine-tune\" the last convolutional block of the VGG16 model alongside the top-level classifier. Fine-tuning consist in starting from a trained network, then re-training it on a new dataset using very small weight updates. In our case, this can be done in 3 steps:\n",
    "\n",
    "*  instantiate the convolutional base of VGG16 and load its weights\n",
    "* add our previously defined fully-connected model on top, and load its weights\n",
    "* freeze the layers of the VGG16 model up to the last convolutional block\n",
    "\n",
    "![](../../images/vgg16_modified.png)\n",
    "\n",
    "Note that:\n",
    "* in order to perform fine-tuning, all layers should start with properly trained weights: for instance you should not slap a randomly initialized fully-connected network on top of a pre-trained convolutional base. This is because the large gradient updates triggered by the randomly initialized weights would wreck the learned weights in the convolutional base. In our case this is why we first train the top-level classifier, and only then start fine-tuning convolutional weights alongside it.\n",
    "* we choose to only fine-tune the last convolutional block rather than the entire network in order to prevent overfitting, since the entire network would have a very large entropic capacity and thus a strong tendency to overfit. The features learned by low-level convolutional blocks are more general, less abstract than those found higher-up, so it is sensible to keep the first few blocks fixed (more general features) and only fine-tune the last one (more specialized features).\n",
    "* fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays very small, so as not to wreck the previously learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# was already done above if you ran all cells\n",
    "# vgg16 = VGG16(include_top=False, weights='imagenet',input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After instantiating the VGG base and loading its weights, we add our previously trained fully-connected classifier on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its necessary to start with a fully-trained classifier including the top classifier to do fine-tuning. Thus we now load the weights we saved in the bottleneck method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model.load_weights('bottleneck_fc_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the model on top of the VGG16 convolutional base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model = Model(inputs=vgg16.input, \n",
    "              outputs=top_model(vgg16.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to freeze all convolutional layers up to the last convolutional block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start training the whole thing, with a very slow learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=2000 // batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach gets us to a validation accuracy of 0.94 after 10 epochs. Great success!\n",
    "\n",
    "Here are a few more approaches you can try to get to above 0.95:\n",
    "* more aggresive data augmentation\n",
    "* more aggressive dropout\n",
    "* use of L1 and L2 regularization (also known as \"weight decay\")\n",
    "* fine-tuning one more convolutional block (alongside greater regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook: Inception V3\n",
    "\n",
    "We can also do the same thing with a more recend ImageNet winner: inception V3, which it actually the one we looked at for these first classifications.\n",
    "\n",
    "For variety, we will only replace the top layer of this network by a single dense layer with one output (cat/dog). For brevity, we will not pre-calculate bootleneck features, but just freeze the whole network. This has the advantage that we can train on the whole augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the pre-trained inception_v3 network without its top layer\n",
    "inception = inception_v3.InceptionV3(include_top=False\n",
    "        , weights='imagenet', input_tensor=None, input_shape=input_shape, pooling=None)\n",
    "\n",
    "# keep the pre-trained layers constant during training\n",
    "for l in inception.layers:\n",
    "    l.trainable = False\n",
    "    \n",
    "# add our own top\n",
    "x = Flatten()(inception.output)\n",
    "predictions=Dense(1, activation='sigmoid')(x)\n",
    "modelInception = Model(outputs=predictions, inputs=inception.input)\n",
    "\n",
    "modelInception.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# modelInception.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model won the 2016 challenge. We will look at it, too, in more detail in a later session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train without augmentation, this shows more gradual improvement\n",
    "modelInception.fit(x_train, y_train\n",
    "                             , batch_size=batch_size\n",
    "                             , validation_data=[x_test, y_test]\n",
    "                             , epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gives us ~98% accuracy after only 10 epochs and not much work.\n",
    "\n",
    "Above we trained without data augmentation just on our bare traning data. With augmentation, the validation accuracy ends up at 99% after a few epochs, value where we might consider our test dataset too small to give us an accurate measurement. The traing accuracy remains smaller, probably because of the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# consider resetting the model above\n",
    "modelInception.fit_generator(train_generator\n",
    "                             , steps_per_epoch=2000 // batch_size\n",
    "                             , validation_data=[x_test, y_test]\n",
    "                             , epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some arbitrary selection from the data\n",
    "predict([1,2000,3000,4000,4300,4510], modelInception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some of the training images\n",
    "predict(range(9), modelInception, data=x_train, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some of the validation images\n",
    "predict(range(9), modelInception, data=x_test, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databias\n",
    "\n",
    "When preparing our dataset we did not include all dog-pictures. In fact, we happened to include only pictures of shepherds and huskies, while another breed was left. See how the inception-based model views these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(range(4400, 4420), modelInception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us tune the model a bit with a more diverse dataset.\n",
    "\n",
    "Here we can also see another way to feed images into keras: directly from directories:\n",
    "\n",
    "## Another Cats-Dogs Dataset\n",
    "\n",
    "We will start from a setup of only 2000 training examples (1000 per class either a dog or a cat). The data is stored in the `data/cats-dogs` folder which has a `train` data directory and a `validation` data directory containing one subdirectory per image class, filled with `.jpg` images:\n",
    "\n",
    "```\n",
    "cats-dogs/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog.001.jpg\n",
    "            dog.002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat.001.jpg\n",
    "            cat.002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog.003.jpg\n",
    "            dog.004.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat.003.jpg\n",
    "            cat.004.jpg\n",
    "            ...\n",
    "```\n",
    "\n",
    "The `validataion` set consists of 400 additional samples from each class as validation data, to evaluate our models.\n",
    "\n",
    "Note that `2000` samples are *very* few examples to learn from, for a classification problem that is far from simple. So this is a challenging machine learning problem, but it is also a realistic one: in a lot of real-world use cases, even small-scale data collection can be extremely expensive or sometimes near-impossible (e.g. in medical imaging). Being able to make the most out of very little data is a key skill of a competent data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = os.path.join('..','..','data','cats-dogs')\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following generator wil read the images from this directory tree and also normalize and augment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen2 = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen2 = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator2 = train_datagen2.flow_from_directory(\n",
    "        train_dir,  # this is the target directory\n",
    "        target_size=(224, 224),  # all images will be resized to 224x224\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "test_generator2 = test_datagen2.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider resetting the model above\n",
    "modelInception.fit_generator(train_generator2\n",
    "                             , steps_per_epoch=2000 // batch_size\n",
    "                             , validation_data=test_generator2\n",
    "                             , validation_steps=800 // batch_size\n",
    "                             , epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can have another look at that misunderstood breed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(range(4400, 4420), modelInception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
